{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "mount_file_id": "https://github.com/HazelvdW/context-framed-listening/blob/main/NLP_framed_listening.ipynb",
      "authorship_tag": "ABX9TyON5Sl18wpmh5soIHABRvhU",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/HazelvdW/context-framed-listening/blob/main/framed_listening_text_prep.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Framed Listening: **Decriptive Stats & Text Preprocessing**\n",
        "> By **Hazel A. van der Walle** (PhD student, Music, Durham University), September 2025."
      ],
      "metadata": {
        "id": "aWUiV_I4GKz5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "All datasets generated and used for this study are openly available on GitHub https://github.com/HazelvdW/context-framed-listening."
      ],
      "metadata": {
        "id": "xIOdxGtRYw4t"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!rm -r context-framed-listening\n",
        "# Clone the GitHub repository\n",
        "!git clone https://github.com/HazelvdW/context-framed-listening.git"
      ],
      "metadata": {
        "id": "h23jkHtYcdel"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Refresh files to see **\"context-framed-listening\"**.\n"
      ],
      "metadata": {
        "id": "uIhxP92zcl9l"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "\n",
        "## Setup"
      ],
      "metadata": {
        "id": "MvLpIyQxZVAo"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bH7jpM6PFif6"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import csv\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "from google.colab import data_table\n",
        "data_table.enable_dataframe_formatter()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Load in the data file \"**data_study1_MAIN**\" that contains participants' thought desciptions"
      ],
      "metadata": {
        "id": "YedXBa_9-UIN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data = pd.read_csv(\"/content/context-framed-listening/data_study1_MAIN.csv\")"
      ],
      "metadata": {
        "id": "P5HImTKOesdJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Sentiment analysis is being conducted on music-evoked thoughts (METs).\n",
        "\n",
        "Create a separate dataset that only contains trials where METs were described (i.e. all rows where \"descr_THOUGHT.text\" is _not_ NA) and drop the columns only relevant to no-MET trials:"
      ],
      "metadata": {
        "id": "G12I7aAPgXJy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dataMET = data[data['descr_THOUGHT.text'].notna()].copy()\n",
        "\n",
        "# drop no-MET columns\n",
        "dataMET.drop(columns = ['response_thought_or_not.keys', 'input_NOT.text'],\n",
        "             inplace=True)\n",
        "\n",
        "# Edit clip_name column\n",
        "for rowIndex, row in dataMET.iterrows():\n",
        "    clip_name = row['clip_name']\n",
        "    if clip_name[0:3] == '80s':\n",
        "        dataMET.loc[rowIndex,'clip_name'] = '80s'+clip_name[3:10]\n",
        "    elif clip_name[0:3] == 'Jaz':\n",
        "        dataMET.loc[rowIndex,'clip_name'] = 'Jaz'+clip_name[4:11]\n",
        "    elif clip_name[0:3] == 'Met':\n",
        "        dataMET.loc[rowIndex,'clip_name'] = 'Met'+clip_name[5:12]\n",
        "    elif clip_name[0:3] == 'Ele':\n",
        "        dataMET.loc[rowIndex,'clip_name'] = 'Ele'+clip_name[10:17]\n",
        "\n",
        "\n",
        "display(dataMET)\n",
        "\n",
        "# print all column headers for later reference\n",
        "print(dataMET.columns)\n",
        "\n",
        "# print number of trials with and without MET descriptions\n",
        "non_na_count = len(dataMET)\n",
        "print(f\"\\nNumber of trials with MET description: {non_na_count}\")\n",
        "\n",
        "na_count = data['descr_THOUGHT.text'].isna().sum()\n",
        "print(f\"Number of trials with no MET description: {na_count}\")"
      ],
      "metadata": {
        "id": "tQVNMRKM468M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Combine the clip and context values into an additional column (`clip_context_PAIR`) Create a clip genre column (`clip_genre`) to use later."
      ],
      "metadata": {
        "id": "AEUdicBMe4Q1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def create_clip_context_pair(row):\n",
        "    clip_name = row['clip_name']\n",
        "    if 'bar' in row['context_word']:\n",
        "        return 'BAR-' + clip_name\n",
        "    elif 'video game' in row['context_word']:\n",
        "        return 'VIDEOGAME-' + clip_name\n",
        "    elif 'concert' in row['context_word']:\n",
        "        return 'CONCERT-' + clip_name\n",
        "    elif 'movie' in row['context_word']:\n",
        "        return 'MOVIE-' + clip_name\n",
        "    else:\n",
        "        return 'NO_MATCH'\n",
        "\n",
        "dataMET['clip_context_PAIR'] = dataMET.apply(create_clip_context_pair, axis=1)\n",
        "\n",
        "# Create 'clip_genre' column\n",
        "def extract_genre(clip_name):\n",
        "    if '80s' in clip_name:\n",
        "        return '80s'\n",
        "    elif 'Jaz' in clip_name:\n",
        "        return 'Jazz'\n",
        "    elif 'Met' in clip_name:\n",
        "        return 'Metal'\n",
        "    elif 'Ele' in clip_name:\n",
        "        return 'Electronic'\n",
        "    else:\n",
        "        return 'UNKNOWN'\n",
        "\n",
        "dataMET['clip_genre'] = dataMET['clip_name'].apply(extract_genre)\n",
        "\n",
        "\n",
        "# Reorder columns\n",
        "cols = dataMET.columns.tolist()\n",
        "# Move 'clip_genre' to be after 'clip_name'\n",
        "cols.insert(cols.index('clip_name') + 1, cols.pop(cols.index('clip_genre')))\n",
        "# Move 'clip_context_PAIR' to be after 'context_word'\n",
        "cols.insert(cols.index('context_word') + 1, cols.pop(cols.index('clip_context_PAIR')))\n",
        "dataMET = dataMET[cols]\n",
        "\n",
        "\n",
        "# Check the dataframe by a quick re-view\n",
        "display(dataMET)\n",
        "print(dataMET.columns)\n",
        "\n",
        "# Saving a .csv for the option to open and look at the full dataframe\n",
        "dataMET.to_csv('/content/context-framed-listening/NLP_outputs/dataMET.csv', encoding='utf-8')"
      ],
      "metadata": {
        "id": "ZrXGur3sfRuH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "## Descriptive Statistics\n",
        "\n",
        "Basic descriptive statistics on the clip-context pairings.\n"
      ],
      "metadata": {
        "id": "9Yis_WyQbX0U"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Create a dataframe including summary info about each clip-context stimuli pairing:\n",
        "\n",
        "* Number of participants that reported METs while listening\n",
        "* Mean MET and clip ratings"
      ],
      "metadata": {
        "id": "yVviYNuV84jD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "columns = dataMET.columns.tolist()[1:-1]\n",
        "\n",
        "# Drop these following columns so they don't aggregate by clip-context grouping:\n",
        "drop = ['clip_name', 'clip_genre', 'context_word', 'clip_context_PAIR',\n",
        "        'expName', 'File_ID', 'date', 'descr_THOUGHT.text',\n",
        "        'demographics.headphones', 'demographics.age',\n",
        "        'demographics.gender','demographics.livingCountry',\n",
        "        'demographics.birthCountry', 'demographics.nativeLanguage',\n",
        "        'demographics.otherLanguage', 'demographics.otherLanguageText',\n",
        "        'demographics.hearingImpariments', 'demographics.hearingImpairmentsText',\n",
        "        'demographics.education','demographics.musicianIdentification',\n",
        "        'demographics.feedback']\n",
        "\n",
        "# Setting up an aggregate function collector\n",
        "agg_fun = {}\n",
        "\n",
        "# As we dropped trials without METs, we can just sum participants for MET occurrence\n",
        "agg_fun['PROLIFIC_PID'] = 'count'\n",
        "\n",
        "# Taking the mean of all columns except participant IDs and dropped columns\n",
        "for col in columns:\n",
        "    if col not in drop and col != 'PROLIFIC_PID':\n",
        "        agg_fun[col] = 'mean'\n",
        "\n",
        "# Group the dataframe by clip-context pairing, then run the aggregate functions created above\n",
        "clipContextDescrStats = dataMET.groupby('clip_context_PAIR').agg(agg_fun)\n",
        "display(clipContextDescrStats)\n",
        "\n",
        "# Saving a .csv for the option to open and look at the full dataframe\n",
        "clipContextDescrStats.to_csv('/content/context-framed-listening/NLP_outputs/clipContextDescrStats.csv', encoding='utf-8')"
      ],
      "metadata": {
        "id": "_Y-L50xzUEJc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Check the minimum and maximum reported MET occurences of all clip-context stimuli pairings:"
      ],
      "metadata": {
        "id": "ukmJdQi8rIST"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "mostMETs_ccpair = clipContextDescrStats['PROLIFIC_PID'].idxmax()\n",
        "leastMETs_ccpair = clipContextDescrStats['PROLIFIC_PID'].idxmin()\n",
        "mostMETs_value = clipContextDescrStats['PROLIFIC_PID'].max()\n",
        "leastMETs_value = clipContextDescrStats['PROLIFIC_PID'].min()\n",
        "\n",
        "print(f\"Clip-context pair with the most reported METs: {mostMETs_ccpair} ({mostMETs_value})\")\n",
        "print(f\"Clip-context pair with the least reported METs: {leastMETs_ccpair} ({leastMETs_value})\")"
      ],
      "metadata": {
        "id": "wDcJ2QiwqBWH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "## Text Preprocessing\n",
        "\n",
        "Different NLP models require different levels of text filtering to perform optimally.\n",
        "\n",
        "**BERT** - Benefits from minimal preprocessing because:\n",
        "* Understands context and handles common words well\n",
        "* Has its own tokenisation and handles word variations\n",
        "* Only needs custom domain-specific stop words removed\n",
        "\n",
        "**Word2Vec & TF-IDF** - Need more aggressive preprocessing because:\n",
        "* Don't understand context as well\n",
        "* Common stop words add noise\n",
        "* Lemmatisation helps group related terms\n",
        "\n",
        " > We will produce two levels of text preprocessing to save out for later analyses. All text will be manually spell checked."
      ],
      "metadata": {
        "id": "nnwzOGTT0-kH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Spell checking\n",
        "\n",
        "Collect all misspellings flagged by spell checking packages to manually go through and make corrections where necessary."
      ],
      "metadata": {
        "id": "50M573SzuV_G"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "from collections import defaultdict\n",
        "\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "\n",
        "from nltk import pos_tag, word_tokenize\n",
        "nltk.download('punkt_tab')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('averaged_perceptron_tagger_eng')\n",
        "\n",
        "#!pip uninstall -y pyspellchecker\n",
        "!pip install pyspellchecker\n",
        "from spellchecker import SpellChecker\n",
        "spell = SpellChecker()"
      ],
      "metadata": {
        "id": "dPJRRRJxtvQ9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def find_potential_misspellings_with_context(row):\n",
        "    misspellings_with_context = []\n",
        "    text = row['descr_THOUGHT.text']\n",
        "    if isinstance(text, str):\n",
        "        # Tokenise the raw text\n",
        "        tokens = word_tokenize(text)\n",
        "        for word in tokens:\n",
        "            # Clean the word for spell checking (lowercase and remove punctuation)\n",
        "            cleaned_word = re.sub(r'[^a-zA-Z0-9]', '', word.lower())\n",
        "            if cleaned_word and cleaned_word not in spell:\n",
        "                misspellings_with_context.append((cleaned_word, text)) # Store word and original text\n",
        "    return misspellings_with_context\n",
        "\n",
        "# Apply the function to each row and collect all misspellings with their context\n",
        "all_misspellings_with_context = []\n",
        "for index, row in dataMET.iterrows():\n",
        "    misspellings_list = find_potential_misspellings_with_context(row)\n",
        "    all_misspellings_with_context.extend(misspellings_list)\n",
        "\n",
        "# Create a DataFrame from the collected misspellings and their context\n",
        "misspellings_df = pd.DataFrame(all_misspellings_with_context, columns=['Potential Misspelling', 'Original Text'])\n",
        "\n",
        "# Display the dataFrame showing unique misspellings\n",
        "## (to show all occurrences, remove .drop_duplicates() <- otherwise, thsi shows their first occurrence context only.)\n",
        "display(misspellings_df.drop_duplicates(subset=['Potential Misspelling']))\n",
        "\n",
        "# Saving the full dataFrame to a CSV file\n",
        "misspellings_df.to_csv('/content/context-framed-listening/NLP_outputs/misspellings_df.csv', encoding='utf-8', index=False)\n",
        "\n",
        "# This dataFrame can be used to create your spelling dictionary."
      ],
      "metadata": {
        "id": "_Tt1zorkuztP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "> I saved the above file out to manually check over an edit the spellings as needed. See `misspelling_correction_ds1.csv`.\n",
        "\n",
        "Below, apply these spelling corrections from the **manually edited csv file** to `dataMET`"
      ],
      "metadata": {
        "id": "1lMV5D1VWFo1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the correction mapping from the CSV file\n",
        "try:\n",
        "    correction_df = pd.read_csv(\"/content/context-framed-listening/NLP_outputs/misspelling_correction_ds1.csv\")\n",
        "\n",
        "    # Ensure the 'correction' column is treated as strings and handle potential NaNs\n",
        "    correction_df['correction'] = correction_df['correction'].astype(str).replace('nan', '') # Convert to string and replace 'nan' string with empty string\n",
        "\n",
        "    # Create a dictionary from the df, filtering out any remaining NaNs if necessary\n",
        "    ## Using .dropna() to remove rows where 'misspelling' is NaN\n",
        "    correction_mapping = pd.Series(correction_df.correction.values, index=correction_df.misspelling).dropna().to_dict()\n",
        "\n",
        "    print(\"Correction mapping loaded successfully.\")\n",
        "    #print(correction_mapping) # [optional viewing]\n",
        "\n",
        "except FileNotFoundError:\n",
        "    print(\"Error: misspelling_correction_ds1.csv not found.\")\n",
        "    correction_mapping = {} # empty mapping if file not found\n",
        "except KeyError:\n",
        "    print(\"Error: CSV must contain 'misspelling' and 'correction' columns.\")\n",
        "    correction_mapping = {} # empty mapping if columns are missing\n",
        "\n",
        "\n",
        "def apply_manual_corrections(text, mapping):\n",
        "    if isinstance(text, str):\n",
        "\n",
        "        # Clean the text (lowercase and remove punctuation) before applying corrections\n",
        "        cleaned_text = re.sub(r'[^a-zA-Z0-9\\s]', '', text.lower())\n",
        "        words = cleaned_text.split()\n",
        "\n",
        "        # Apply corrections\n",
        "        ## Ensure the mapped value is a string before joining\n",
        "        corrected_words = [str(mapping.get(word, word)) for word in words]\n",
        "        return \" \".join(corrected_words)\n",
        "    return str(text) # Also ensure the return value is a string if the input was not a string\n",
        "\n",
        "# Apply the manual corrections to the descr_THOUGHT.text column\n",
        "dataMET['descr_THOUGHT.text_corrected'] = dataMET['descr_THOUGHT.text'].apply(lambda x: apply_manual_corrections(x, correction_mapping))\n",
        "\n",
        "# Display the original and corrected text to check the changes\n",
        "display(dataMET[['descr_THOUGHT.text', 'descr_THOUGHT.text_corrected']])\n",
        "\n",
        "# Now, the preprocessed_MET_descr column in the next cell should use descr_THOUGHT.text_corrected as input"
      ],
      "metadata": {
        "id": "1hkm_BdEwOIO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "Importing some more packages necessary for text cleaning before feeding these METs into our NLP models."
      ],
      "metadata": {
        "id": "V5QHhVlMsnTq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Stop Words\n",
        "\n",
        "These are word we will remove from the text that won't be acounted for in analyses.\n",
        "\n",
        "We will define custom domain-specific stop words (e.g. music stlye, context cues, thought types), and get NLTK common English-language stop words.\n",
        "\n",
        "Both preprocessing levels will have custom stop words removed, but only level 2 preprocessing for Word2Vec and TF-IDF will have the NLTK stop words removed (as BERT understands \"the\", \"is\", \"and\", etc.)."
      ],
      "metadata": {
        "id": "WlM-J2JUsKgB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define custom stop words (domain-specific terms to remove)\n",
        "customStopWords = ['music', 'song', 'songs', 'excerpt', 'excerpts', 'piece', 'pieces', 'clip', 'clips',\n",
        "                   'electronic', 'jazz', 'metal', 'rock',\n",
        "                   'bar', 'concert', 'film', 'movie', 'videogame', 'video', 'game',\n",
        "                   #'nineteen', '1920s', '20s', '1930s', '30s', '1940s', '40s',\n",
        "                   #'50s', '1950s', 'fifties', '50', '1950', 'fifty',\n",
        "                   #'60s', '1960s', 'sixties', '60', '1960', 'sixty',\n",
        "                   #'70s', '1970s', 'seventies', '70', '1970', 'seventy',\n",
        "                   #'80s', '1980s', 'eighties', '80', '1980', 'eighty',\n",
        "                   #'90s', '1990s', 'nineties', '90', '1990', 'ninety',\n",
        "                   #'00s', '2000s', 'noughties', '2000', 'y2k', '2010',\n",
        "                   'think', 'thinks', 'thought', 'thinking',\n",
        "                   'imagine', 'imagines', 'imagined', 'imagining',\n",
        "                   'image', 'images', 'imaged', 'imaging',\n",
        "                   'visualise', 'visualises', 'visualised', 'visualising',\n",
        "                   'visualize', 'visualizes', 'visualized', 'visualizing',\n",
        "                   'picture', 'pictures', 'pictured', 'picturing',\n",
        "                   'scene', 'scenes', 'story', 'stories',\n",
        "                   'memory', 'memories', 'reminder', 'reminders', 'remind', 'reminds',\n",
        "                   'remember', 'remembers', 'remembered', 'remembering',\n",
        "                   'reminiscent', 'reminisce', 'reminisces', 'reminisced', 'reminiscing',\n",
        "                   'make', 'makes', 'made', 'making',\n",
        "                   'sound', 'sounds', 'sounded', 'sounding']\n",
        "\n",
        "\n",
        "# Combine custom stop words with NLTK stop words\n",
        "nltk_stop_words = set(nltk.corpus.stopwords.words('english'))\n",
        "print(nltk_stop_words)\n",
        "\n",
        "all_stop_words = set(customStopWords).union(nltk_stop_words)\n",
        "print(\"\\nCustom Stop Words:\", len(customStopWords))\n",
        "print(\"Total Stop Words (custom + NLTK):\", len(all_stop_words))"
      ],
      "metadata": {
        "id": "DxofLkrplN8N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Lemmatise\n",
        "\n",
        "Lemmatising groups inflected word forms together by identifying the dictionary form (the lemma) to analyse them as a single item.\n",
        "\n",
        "[_e.g., improve/improving/improvements/improved/improver = improve_]\n",
        "\n",
        "Only level 2 preprocessing for Word2Vec and TF-IDF will have lemmatisation (BERT is able to handle word variations)."
      ],
      "metadata": {
        "id": "ORSYH4JPswTy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialise lemmatiser and tag mapping\n",
        "\n",
        "lemmatiser = nltk.stem.WordNetLemmatizer()\n",
        "wn = nltk.corpus.wordnet\n",
        "\n",
        "tag_map = defaultdict(lambda: wn.NOUN)\n",
        "tag_map['J'] = wn.ADJ\n",
        "tag_map['V'] = wn.VERB\n",
        "tag_map['R'] = wn.ADV"
      ],
      "metadata": {
        "id": "dQ4BcWcKuF2X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Apply text preprocessing:\n",
        "\n"
      ],
      "metadata": {
        "id": "g1V_XOhN8u19"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "20eb3862"
      },
      "source": [
        "def preprocess_level1(text, custom_stop_words):\n",
        "    \"\"\"\n",
        "    Level 1: Minimal preprocessing for BERT\n",
        "    - Only removes custom domain-specific stop words\n",
        "    - Preserves original word forms (no lemmatisation)\n",
        "    - Keeps punctuation and capitalization for BERT's tokeniser\n",
        "    \"\"\"\n",
        "    if isinstance(text, str):\n",
        "        # Tokenise the text\n",
        "        tokens = word_tokenize(text)\n",
        "        # Only remove custom stop words, keep everything else\n",
        "        filtered_tokens = []\n",
        "        for word in tokens:\n",
        "            # Check lowercase version against stop words, but keep original case\n",
        "            if word.lower() not in custom_stop_words:\n",
        "                filtered_tokens.append(word)\n",
        "        return \" \".join(filtered_tokens)\n",
        "    return text\n",
        "\n",
        "\n",
        "def preprocess_level2(text, all_stop_words, lemmatiser, tag_map):\n",
        "    \"\"\"\n",
        "    Level 2: Aggressive preprocessing for Word2Vec and TF-IDF\n",
        "    - Removes all stop words (custom + NLTK)\n",
        "    - Applies lemmatisation\n",
        "    - Converts to lowercase\n",
        "    - Removes punctuation\n",
        "    \"\"\"\n",
        "    if isinstance(text, str):\n",
        "        # Tokenise the text\n",
        "        tokens = word_tokenize(text)\n",
        "        # Remove stop words and lemmatize\n",
        "        lemmatised_tokens = []\n",
        "        for word, tag in pos_tag(tokens):\n",
        "            # Convert to lowercase and remove non-alphabetic characters\n",
        "            cleaned_word = re.sub(r'[^a-zA-Z]', '', word.lower())\n",
        "            if cleaned_word and cleaned_word not in all_stop_words:\n",
        "                # Lemmatize using the POS tag\n",
        "                lemmatised_word = lemmatiser.lemmatize(cleaned_word, tag_map[tag[0]])\n",
        "                lemmatised_tokens.append(lemmatised_word)\n",
        "        return \" \".join(lemmatised_tokens)\n",
        "    return text\n",
        "\n",
        "\n",
        "# Apply Level 1 preprocessing (for BERT)\n",
        "dataMET['METdescr_prepLVL1'] = dataMET['descr_THOUGHT.text_corrected'].apply(\n",
        "    lambda x: preprocess_level1(x, set(customStopWords))\n",
        ")\n",
        "\n",
        "# Apply Level 2 preprocessing (for Word2Vec and TF-IDF)\n",
        "dataMET['METdescr_prepLVL2'] = dataMET['descr_THOUGHT.text_corrected'].apply(\n",
        "    lambda x: preprocess_level2(x, all_stop_words, lemmatiser, tag_map)\n",
        ")\n",
        "\n",
        "# Display all text processing stages\n",
        "display(dataMET[['descr_THOUGHT.text',\n",
        "                 'descr_THOUGHT.text_corrected',\n",
        "                 'METdescr_prepLVL1',\n",
        "                 'METdescr_prepLVL2']])\n",
        "\n",
        "# Save out for analyses and inspection\n",
        "dataMET.to_csv('/content/context-framed-listening/NLP_outputs/dataMET_preprocessed.csv', encoding='utf-8')\n",
        "\n",
        "print(\"\\nPreprocessing complete!\")\n",
        "print(\"- 'METdescr_prepLVL1': Minimal filtering for BERT (custom stop words only)\")\n",
        "print(\"- 'METdescr_prepLVL2': Full preprocessing for Word2Vec/TF-IDF (all stop words + lemmatisation)\")\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "`dataMET` (saved out as `dataMET_preprocessed.csv`) now contains the preprocessed dataframe with the two levels of filtering: `METdata_prepLVL1` and `METdata_prepLVL2`. We will refer to this file for the following NLP models."
      ],
      "metadata": {
        "id": "wsytbH07Sgf4"
      }
    }
  ]
}